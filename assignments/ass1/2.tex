\section{Question 2}

\begin{question}
    Show that the shortest distance from an arbitrary point to an arbitrary line through the origin in \mathbb{R}^2 is the length of a vector orthogonal to any vector in the line. Parametrize the line as {k[a,b]T: k âˆˆR}.
\end{question}

\begin{answer}
    \begin{proof}
        Since we know $\overline{y} = \tfrac{\sum_{i = 1}^{n}{y_i}}{n}$ and $\overline{x} = \tfrac{\sum_{i = 1}^{n}{x_i}}{n}$, then we can substitute these $\overline{x}$ into the regression line, then we have:
        \begin{align}
            y &= \hat{\beta_0} + \hat{\beta_1}\left(\dfrac{\sum_{i = 1}^{n}{x_i}}{n}\right)\\
            & = \dfrac{\hat{\beta_0} + \hat{\beta_1}\sum_{i = 1}^{n}{x_i}}{n}\\
            & = \dfrac{\sum_{i = 1}^{n}{\hat{\beta_0} + \hat{\beta_1}x_i}}{n}\\
            & = \dfrac{\sum_{i = 1}^{n}{y_i}}{n}\; \text{  (Since } y_i = \hat{\beta_0} + \hat{\beta_1}x_i \text{)}\\
            & = \overline{y}
        \end{align}
        This shows that the point $(\overline{x},\overline{y})$ is always on the regression line 
        \begin{center}
            $y = \hat{\beta_0} + \hat{\beta_1}x$
        \end{center}. 
    \end{proof}
\end{answer}
